{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LORA\n",
    "* Model: distilbert-base-uncased\n",
    "* Evaluation approach: Hugging Face Trainer class\n",
    "* Fine-tuning dataset:fancyzhx/amazon_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#The following code in the three sections (Loading and Evaluating the model, Performing Parameter-Efficient Fine-Tuning \n",
    "#and Performing Inference with a PEFT Model)was generated with the assistance from VOX chat gpt-35-turbo-16k (8-11 Dec-2024),\n",
    "#GPT-4 from OpenAI (8-11 Dec-2024), and MicrosoftCopilot(8-11 Dec-2024)\n",
    "\n",
    "#Code from the UDACITY GenAI fundamentals was also taken as reference to complete the code.\n",
    "#Specific errors solutions are specified with the URL in the steps that were needed\n",
    "\n",
    "#*****************************************\n",
    "\n",
    "#Load a pretrained HF Distillbert model\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2, #Binary classification, 1 = positive, 0 = negative\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # For converting predictions to strings\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenization column names: ['label', 'title', 'content']\n",
      "{'label': 0, 'title': 'Anyone who likes this better than the Pekinpah is a moron.', 'content': \"All the pretty people in this film. Even the Rudy character played by Michael Madsen. This is adapted from a Jim Thompson novel for cryin' out loud! These are supposed to be marginal characters, not fashion models. Though McQueen and McGraw were attractive (but check out McQueen's crummy prison haircut) they were believable in the role. Baldwin and Bassinger seem like movie stars trying to act like hard cases. Action wise, the robbery scene in the Pekinpah version was about 100 times more exciting and suspenseful than anything in this re-make.\"}\n"
     ]
    }
   ],
   "source": [
    "#Tokenize data set\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Tokenize data set\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load the train and test splits of the dataset\n",
    "dataset = load_dataset(\"fancyzhx/amazon_polarity\")\n",
    "\n",
    "# Create a smaller subset for train and test- Copilot helped\n",
    "subset_size = min(5000, len(dataset[\"train\"]), len(dataset[\"test\"]))\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(subset_size))\n",
    "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(subset_size))\n",
    "\n",
    "print(\"Before tokenization column names:\", small_train_dataset.column_names) #Printed collumns before tokenization to check whats happening\n",
    "\n",
    "print(small_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization column name: ['label', 'title', 'content', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize data. Copilot helped\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"content\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = small_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"After tokenization column name:\", tokenized_train_dataset.column_names) #Printed collumns after tokenization to check whats happening\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f28c4a78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 5000\n",
      "Number of test samples: 5000\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/78031519/how-to-resolve-valueerror-you-should-supply-an-encoding-or-a-list-of-encodings\n",
    "train_lora = tokenized_train_dataset.rename_column('label', 'labels')\n",
    "test_lora = tokenized_test_dataset.rename_column('label', 'labels')\n",
    "\n",
    "#Remove unnecessary columns- Copilot helped since the error of not providing inputs-id keept coming \n",
    "train_lora = train_lora.remove_columns(['title', 'content'])\n",
    "test_lora = test_lora.remove_columns(['title', 'content'])\n",
    "\n",
    "# print(\"After renaming and removing unnecessary columns:\", train_lora.column_names) #Printed collumns after removing to check whats happening\n",
    "\n",
    "print(f\"Number of training samples: {len(train_lora)}\")\n",
    "print(f\"Number of test samples: {len(test_lora)}\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_lora, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_lora, batch_size=16)\n",
    "\n",
    "# Check the first batch\n",
    "# for batch in train_dataloader:\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the compute_metrics for evaluating accuracy of the foundational model prior training. Copilot and pdf support document from George Stamos helped\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\".\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=1, # Reduce batch size for not collapsing the memory\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=0,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=train_lora,\n",
    "    eval_dataset=test_lora,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer,\n",
    "    padding=\"max_length\"),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6975525617599487, 'eval_accuracy': 0.4774, 'eval_runtime': 92.3941, 'eval_samples_per_second': 54.116, 'eval_steps_per_second': 54.116}\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model prior training\n",
    "print(\"Evaluating the model before fine-tuning:...\")\n",
    "pre_fine_tuning = trainer.evaluate()\n",
    "print(pre_fine_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad0ce8",
   "metadata": {},
   "source": [
    "# Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): MultiHeadSelfAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.01, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.01, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Creating a PEFT Config https://huggingface.co/docs/peft/package_reference/config\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification, TaskType\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "#Copilot helped with the correct target_modules\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, #Task type for sequence classification\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    inference_mode=False, \n",
    "    target_modules=[\"q_lin\", \"v_lin\"], #focusing solely on the linear transformations related to the query and passage encoding steps. As copilot suggestions, since using also (k_lin) was causing and error of not providng inputs-id\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "#8-bit optimizers:\n",
    "adam = bnb.optim.Adam(lora_model.parameters(), \n",
    "                      lr=0.001, \n",
    "                      betas=(0.9, 0.995), \n",
    "                      optim_bits=32, \n",
    "                      percentile_clipping=5)\n",
    "\n",
    "print(lora_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "894046c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Train the PEFT model with Copilot help\n",
    "#Prepare Data Loaders: Create data loaders for training and evaluation:\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, default_data_collator, DataCollatorWithPadding, TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    loss = eval_pred[0]\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy, \"eval_loss\": loss}\n",
    "\n",
    "# Custom callback to log evaluation metrics. Copilot suggestion for the purpose to perform additional actions during training or evaluation at specific points.\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        metrics = kwargs.get(\"metrics\", {})\n",
    "        print(f\"Evaluation metrics at step {state.global_step}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a02bddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [939/939 13:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.273215</td>\n",
       "      <td>0.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.316800</td>\n",
       "      <td>0.285620</td>\n",
       "      <td>0.895200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.316800</td>\n",
       "      <td>0.350202</td>\n",
       "      <td>0.904600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics at step 313: {'eval_loss': 0.2732151746749878, 'eval_accuracy': 0.8934, 'eval_runtime': 85.5653, 'eval_samples_per_second': 58.435, 'eval_steps_per_second': 3.658, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-313 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics at step 626: {'eval_loss': 0.2856197655200958, 'eval_accuracy': 0.8952, 'eval_runtime': 85.5968, 'eval_samples_per_second': 58.413, 'eval_steps_per_second': 3.657, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-626 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics at step 939: {'eval_loss': 0.3502024710178375, 'eval_accuracy': 0.9046, 'eval_runtime': 85.451, 'eval_samples_per_second': 58.513, 'eval_steps_per_second': 3.663, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-939 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=939, training_loss=0.25343988193109773, metrics={'train_runtime': 793.6188, 'train_samples_per_second': 18.901, 'train_steps_per_second': 1.183, 'total_flos': 2021091102720000.0, 'train_loss': 0.25343988193109773, 'epoch': 3.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the Trainer\n",
    "# for load_best_model_at_end=False to correct eval loss error taken from stackoverflow blog\n",
    "#remove_unused_columns=False,https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\".\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=False,\n",
    "        remove_unused_columns=False, #https://stackoverflow.com/questions/76796304/invalid-key-409862-is-out-of-bounds-for-size-0\n",
    "       \n",
    "    ),\n",
    "    train_dataset=train_lora, \n",
    "    eval_dataset=test_lora,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=\"max_length\"),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CustomCallback],  # Add the custom callback here\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,331,716 || all params: 67,694,596 || trainable%: 1.967241225577297\n",
      "Evaluating the model after fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 01:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics at step 939: {'eval_loss': 0.3502024710178375, 'eval_accuracy': 0.9046, 'eval_runtime': 85.759, 'eval_samples_per_second': 58.303, 'eval_steps_per_second': 3.65, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3502024710178375, 'eval_accuracy': 0.9046, 'eval_runtime': 85.759, 'eval_samples_per_second': 58.303, 'eval_steps_per_second': 3.65, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "#Checking Trainable Parameters of a PEFT Model\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Evaluate the model after fine-tuning\n",
    "print(\"Evaluating the model after fine-tuning...\")\n",
    "\n",
    "post_finetune_eval = trainer.evaluate()\n",
    "\n",
    "print(post_finetune_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#Save the PEFT model\n",
    "lora_model.save_pretrained(\"distilbert-base-uncased-model-lora_model\")\n",
    "\n",
    "#confirmed that the model was saved. Copilot helped to check\n",
    "import os\n",
    "\n",
    "save_directory = \"distilbert-base-uncased-model-lora_model\"\n",
    "file_list = os.listdir(save_directory)\n",
    "\n",
    "if len(file_list) > 0:\n",
    "    print(\"Model saved successfully.\")\n",
    "else:\n",
    "    print(\"Model saving failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): MultiHeadSelfAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.01, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.01, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Load the Pretrained Model\n",
    "\n",
    "from peft import get_peft_model, AutoPeftModelForSequenceClassification\n",
    "\n",
    "# Load the saved PEFT model\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-model-lora_model\")\n",
    "\n",
    "\n",
    "lora_model.eval()\n",
    "\n",
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute_metrics. Copilot helped\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for PEFT model\n",
    "trainer_peft = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\".\",\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        remove_unused_columns=False,\n",
    "    ),\n",
    "    eval_dataset=test_lora,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=\"max_length\"),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the loaded fine-tuned model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 01:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the loaded fine-tuned Lora_model Results: {'eval_loss': 0.3502024710178375, 'eval_accuracy': 0.9046, 'eval_runtime': 97.9864, 'eval_samples_per_second': 51.027, 'eval_steps_per_second': 51.027}\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the Performance of the Trained PEFT Model. The accuracy did not improve dramatically and it might need more epochs:\n",
    "print(\"Evaluating the loaded fine-tuned model...\")\n",
    "results_peft = trainer_peft.evaluate()\n",
    "print(\"Evaluating the loaded fine-tuned Lora_model Results:\", results_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior-training model Results: {'eval_loss': 0.6942567825317383, 'eval_accuracy': 0.4852, 'eval_runtime': 92.9459, 'eval_samples_per_second': 53.795, 'eval_steps_per_second': 53.795}\n"
     ]
    }
   ],
   "source": [
    "# Assuming results from prior evaluation are stored in `results`\n",
    "# Compare with the foundation model results\n",
    "print(\"Prior-training model Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
